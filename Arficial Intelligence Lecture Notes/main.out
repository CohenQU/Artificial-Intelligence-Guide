\BOOKMARK [1][-]{section.0.1}{Introduction}{}% 1
\BOOKMARK [2][-]{subsection.0.1.1}{Definition}{section.0.1}% 2
\BOOKMARK [2][-]{subsection.0.1.2}{Basic Idea}{section.0.1}% 3
\BOOKMARK [3][-]{subsubsection.0.1.2.1}{Generate-And-Test Algorithm}{subsection.0.1.2}% 4
\BOOKMARK [3][-]{subsubsection.0.1.2.2}{Rumpelstiltsin Principle}{subsection.0.1.2}% 5
\BOOKMARK [3][-]{subsubsection.0.1.2.3}{Simple = Trival}{subsection.0.1.2}% 6
\BOOKMARK [1][-]{section.0.2}{Reasoning: Goal Trees and Problem Solving}{}% 7
\BOOKMARK [2][-]{subsection.0.2.1}{Question}{section.0.2}% 8
\BOOKMARK [2][-]{subsection.0.2.2}{Model}{section.0.2}% 9
\BOOKMARK [3][-]{subsubsection.0.2.2.1}{Problem Rediction or AND-OR Tree or Goal Tree}{subsection.0.2.2}% 10
\BOOKMARK [3][-]{subsubsection.0.2.2.2}{Architecture}{subsection.0.2.2}% 11
\BOOKMARK [2][-]{subsection.0.2.3}{Transformation}{section.0.2}% 12
\BOOKMARK [3][-]{subsubsection.0.2.3.1}{Safe Transformation}{subsection.0.2.3}% 13
\BOOKMARK [3][-]{subsubsection.0.2.3.2}{Heuristic Transformation}{subsection.0.2.3}% 14
\BOOKMARK [3][-]{subsubsection.0.2.3.3}{Procedure}{subsection.0.2.3}% 15
\BOOKMARK [2][-]{subsection.0.2.4}{Reflection}{section.0.2}% 16
\BOOKMARK [2][-]{subsection.0.2.5}{Questions about the nature of knowledge}{section.0.2}% 17
\BOOKMARK [1][-]{section.0.3}{Reasoning: Goal Trees and Rule-Based Expert Systems}{}% 18
\BOOKMARK [2][-]{subsection.0.3.1}{Goal Centered Programming}{section.0.3}% 19
\BOOKMARK [3][-]{subsubsection.0.3.1.1}{Goal}{subsection.0.3.1}% 20
\BOOKMARK [3][-]{subsubsection.0.3.1.2}{Function}{subsection.0.3.1}% 21
\BOOKMARK [3][-]{subsubsection.0.3.1.3}{Procedure}{subsection.0.3.1}% 22
\BOOKMARK [2][-]{subsection.0.3.2}{Rule-Based Expert System}{section.0.3}% 23
\BOOKMARK [1][-]{section.0.4}{Search: Depth-First, Hill Climbing, Beam}{}% 24
\BOOKMARK [2][-]{subsection.0.4.1}{Question}{section.0.4}% 25
\BOOKMARK [2][-]{subsection.0.4.2}{British Museum Approach\205find every possible path}{section.0.4}% 26
\BOOKMARK [2][-]{subsection.0.4.3}{Depth First Search/Breadth First Search}{section.0.4}% 27
\BOOKMARK [2][-]{subsection.0.4.4}{Hill Climbing}{section.0.4}% 28
\BOOKMARK [2][-]{subsection.0.4.5}{Beam Search}{section.0.4}% 29
\BOOKMARK [2][-]{subsection.0.4.6}{Procedure}{section.0.4}% 30
\BOOKMARK [2][-]{subsection.0.4.7}{Property}{section.0.4}% 31
\BOOKMARK [1][-]{section.0.5}{Search: Optimal, Branch and Bound, A*}{}% 32
\BOOKMARK [2][-]{subsection.0.5.1}{Question: find the best path}{section.0.5}% 33
\BOOKMARK [2][-]{subsection.0.5.2}{Branch \046 Bound}{section.0.5}% 34
\BOOKMARK [2][-]{subsection.0.5.3}{with Extended List}{section.0.5}% 35
\BOOKMARK [2][-]{subsection.0.5.4}{with Admissible Heuristic}{section.0.5}% 36
\BOOKMARK [2][-]{subsection.0.5.5}{A*}{section.0.5}% 37
\BOOKMARK [1][-]{section.0.6}{Search: Games, Minimax, and Alpha-Beta}{}% 38
\BOOKMARK [2][-]{subsection.0.6.1}{Ways to Play}{section.0.6}% 39
\BOOKMARK [2][-]{subsection.0.6.2}{Minimax Algorithm}{section.0.6}% 40
\BOOKMARK [2][-]{subsection.0.6.3}{Alpha-Beta Pruning}{section.0.6}% 41
\BOOKMARK [2][-]{subsection.0.6.4}{Progressive Deepening\205Anytime Algorithm}{section.0.6}% 42
\BOOKMARK [2][-]{subsection.0.6.5}{Deep Blue}{section.0.6}% 43
\BOOKMARK [1][-]{section.0.7}{Constraints: Interpreting Line Drawings}{}% 44
\BOOKMARK [2][-]{subsection.0.7.1}{Guzman's Solution}{section.0.7}% 45
\BOOKMARK [2][-]{subsection.0.7.2}{Dave Huffman's Solution}{section.0.7}% 46
\BOOKMARK [2][-]{subsection.0.7.3}{David Waltz's Solution}{section.0.7}% 47
\BOOKMARK [1][-]{section.0.8}{Constraints: Search, Domain Reduction}{}% 48
\BOOKMARK [2][-]{subsection.0.8.1}{Term}{section.0.8}% 49
\BOOKMARK [2][-]{subsection.0.8.2}{Procedure\205Domain Reduction Algorithm}{section.0.8}% 50
\BOOKMARK [2][-]{subsection.0.8.3}{Consider}{section.0.8}% 51
\BOOKMARK [1][-]{section.0.9}{Constraints: Visual Object Recognition}{}% 52
\BOOKMARK [2][-]{subsection.0.9.1}{David Marr's idea\205JUST A IDEA}{section.0.9}% 53
\BOOKMARK [2][-]{subsection.0.9.2}{Shimon Ullman\205Alignment Theory}{section.0.9}% 54
\BOOKMARK [2][-]{subsection.0.9.3}{Correlation Theory}{section.0.9}% 55
\BOOKMARK [1][-]{section.0.10}{Introduction to Learning, Nearest Neighbors}{}% 56
\BOOKMARK [2][-]{subsection.0.10.1}{Types of Learning}{section.0.10}% 57
\BOOKMARK [2][-]{subsection.0.10.2}{Mechanism of Pattern Recognition}{section.0.10}% 58
\BOOKMARK [2][-]{subsection.0.10.3}{Learning}{section.0.10}% 59
\BOOKMARK [1][-]{section.0.11}{Learning: Identification Trees, Disorder}{}% 60
\BOOKMARK [2][-]{subsection.0.11.1}{Target}{section.0.11}% 61
\BOOKMARK [2][-]{subsection.0.11.2}{Difference between the Dataset of Identification Tree and Nearest Neighbor}{section.0.11}% 62
\BOOKMARK [2][-]{subsection.0.11.3}{Procedure\204\204Occam's Razor}{section.0.11}% 63
\BOOKMARK [3][-]{subsubsection.0.11.3.1}{Naive Strategy}{subsection.0.11.3}% 64
\BOOKMARK [3][-]{subsubsection.0.11.3.2}{Large Dataset}{subsection.0.11.3}% 65
\BOOKMARK [2][-]{subsection.0.11.4}{Decision Boundary}{section.0.11}% 66
\BOOKMARK [2][-]{subsection.0.11.5}{Tree to Rule}{section.0.11}% 67
\BOOKMARK [1][-]{section.0.12}{Neural Nets}{}% 68
\BOOKMARK [2][-]{subsection.0.12.1}{Model Real Neuron}{section.0.12}% 69
\BOOKMARK [2][-]{subsection.0.12.2}{Training}{section.0.12}% 70
\BOOKMARK [2][-]{subsection.0.12.3}{Gradient Descent}{section.0.12}% 71
\BOOKMARK [2][-]{subsection.0.12.4}{}{section.0.12}% 72
\BOOKMARK [2][-]{subsection.0.12.5}{Backpropagation Algorithm}{section.0.12}% 73
\BOOKMARK [1][-]{section.0.13}{Learning: Genetic Algorithms}{}% 74
\BOOKMARK [2][-]{subsection.0.13.1}{Imitation \(ATCG -3mu 01\)}{section.0.13}% 75
\BOOKMARK [2][-]{subsection.0.13.2}{Mechanism}{section.0.13}% 76
\BOOKMARK [3][-]{subsubsection.0.13.2.1}{Fitness}{subsection.0.13.2}% 77
\BOOKMARK [3][-]{subsubsection.0.13.2.2}{Rank Space}{subsection.0.13.2}% 78
\BOOKMARK [3][-]{subsubsection.0.13.2.3}{Diversity}{subsection.0.13.2}% 79
\BOOKMARK [1][-]{section.0.14}{Learning: Sparse Spaces, Phonology}{}% 80
\BOOKMARK [2][-]{subsection.0.14.1}{Point}{section.0.14}% 81
\BOOKMARK [2][-]{subsection.0.14.2}{Phonological Rules}{section.0.14}% 82
\BOOKMARK [3][-]{subsubsection.0.14.2.1}{Distinctive features Theory}{subsection.0.14.2}% 83
\BOOKMARK [3][-]{subsubsection.0.14.2.2}{YIP-SUSSMAN Machine/Learner}{subsection.0.14.2}% 84
\BOOKMARK [2][-]{subsection.0.14.3}{Sparse Spaces\(!More information need\)}{section.0.14}% 85
\BOOKMARK [2][-]{subsection.0.14.4}{Marr's Catechism}{section.0.14}% 86
\BOOKMARK [1][-]{section.0.15}{Learning: Near Misses, Felicity Conditions}{}% 87
\BOOKMARK [2][-]{subsection.0.15.1}{Learning Process}{section.0.15}% 88
\BOOKMARK [2][-]{subsection.0.15.2}{Five Qualities}{section.0.15}% 89
\BOOKMARK [1][-]{section.0.16}{Learning: Support Vector Machines}{}% 90
\BOOKMARK [2][-]{subsection.0.16.1}{Decision Boundary}{section.0.16}% 91
\BOOKMARK [2][-]{subsection.0.16.2}{Vladimir Vapnik's Idea\205Support Vector Machine}{section.0.16}% 92
\BOOKMARK [2][-]{subsection.0.16.3}{Kernal Function}{section.0.16}% 93
\BOOKMARK [1][-]{section.0.17}{Learning: Boosting}{}% 94
\BOOKMARK [1][-]{section.0.18}{Representations: Classes, Trajectories, Transitions}{}% 95
\BOOKMARK [1][-]{section.0.19}{Architectures: GPS, SOAR, Subsumption, Society of Mind}{}% 96
\BOOKMARK [1][-]{section.0.20}{Probabilistic Inference I}{}% 97
\BOOKMARK [1][-]{section.0.21}{Probabilistic Inference II}{}% 98
\BOOKMARK [1][-]{section.0.22}{Model Merging, Cross-Modal Coupling, Course Summary}{}% 99

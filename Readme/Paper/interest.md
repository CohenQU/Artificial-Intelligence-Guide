* [Adversarial Training Methods for Semi-Supervised Text Classification](https://arxiv.org/abs/1605.07725)
* [Synthetic and Natural Noise Both Break Neural Machine Translation](https://arxiv.org/abs/1711.02173)
* [Adversarial Reprogramming of Neural Networks](https://arxiv.org/abs/1806.11146)
* [DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks](https://arxiv.org/pdf/1511.04599)

## NAACL 2019 Outstanding Papers

* Best Thematic Paper: [What's in a Name? Reducing Bias in Bios without Access to Protected Attributes](https://arxiv.org/abs/1904.05233)
* Best Explainable NLP Paper: [CNM: An Interpretable Complex-valued Network for Matching](https://arxiv.org/abs/1904.05298)
* Best Long Paper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
* Best Short Paper: [Probing the Need for Visual Context in Multimodal Machine Translation](https://arxiv.org/abs/1903.08678)
* Best Resource Paper: [CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge](https://arxiv.org/abs/1811.00937)

## NAACL 2018 Outstanding Papers

* [Deep Contextualized Word Representations](https://arxiv.org/abs/1802.05365)
* [Learning to Map Context-Dependent Sentences to Executable Formal Queries](https://arxiv.org/abs/1804.06868)
* [Neural Text Generation in Stories using Entity Representations as Context](https://aclweb.org/anthology/papers/N/N18/N18-1204/)
* [Recurrent Neural Networks as Weighted Language Recognizers](https://arxiv.org/abs/1711.05408)

## Word Embedding

* NNLM: [A Neural Probabilistic Language Model](https://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model.pdf)
* Word2Vec: [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
* GloVe: [GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162)
* ELMo: [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)
* GPT: [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
* BERT: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
